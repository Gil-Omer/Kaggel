---
title: "Kaggle_ML4eco"
author: "Gil Omer and Doron Zamir"
date: "6/19/2021"
output: html_document
---

```{r setup, include=FALSE}
#rm(list=ls())
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval    = TRUE,
  echo    = TRUE,
  warning = FALSE,
  message = FALSE,
  cache   = FALSE,
  dev     = "svglite",
  fig.ext = ".svg"
)
```

# Packages and setup

```{r pacman, message=FALSE, warning=FALSE, eval=TRUE}
pacman::p_load(
  tidyverse,   # for data wrangling and visualization
  tidymodels,  # for data modeling
  GGally,      # for pairs plot
  skimr,       # for summary statistics
  here,         # for referencing folders and files
  broom,  # for tidy model output
  rpart, # for estimating CART
  rpart.plot, # for plotting rpart objects
  ranger, # for estimating random forests
  vip, # for variable importance plots
  DataExplorer,
  glmnet,
  ggmosaic
  )

```

## read data

```{r}
wage_train <- 
  here("train.csv") %>% 
  read_csv()
wage_test <- 
  here("test.csv") %>% 
  read_csv()

```

# Data Analasys

## explore data - glimpse

```{r glimpse}
glimpse(wage_train)
wage_train %>%skim()

```

We can see that variables female, college degree, advanced degree,region, race and occupation dummies, should be factors.\
In addition, we can see there are no farmers in the train set.

## Histograms

```{r histogram}
plot_histogram(wage_train)

```

We can see that years of experience (full time) is more spread (from 0 to 45) compared to part time (most observations around 0). This can be explained if most people work in part time jobs for a short period (before/after college for example) but move on to full time jobs. Hence, we want to look at the total education years+exp years (=~age).

## Boxplot of every variable against log wage

```{r boxplot}
plot_boxplot(wage_train,"lnwage")
```

we can see that there is more variance in higher education levels and in very high and small experience (full time). we can see that higher wage is more associated with low part time experience.

# Modeling

## Basic Model

This model should be used as a benchmark

### Basic Recipe

```{r}
base_rec <-
  recipe(lnwage ~., data = wage_train) %>% 
  update_role(ID, new_role = "ID") %>% 
  step_bin2factor(all_predictors(), -starts_with("exp"), -"edyrs" ) %>%
  step_mutate(age = edyrs + expf + expp) %>%
  step_poly(age, degree = tune("lambda")) %>%
  step_poly(edyrs, degree = tune("delta")) %>%
  step_poly(expfsq, degree = tune("gamma")) %>%
  step_center(all_predictors(), - all_nominal()) %>%
  step_scale(all_predictors(), - all_nominal()) %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors())
```

### Set cross - Validation

```{r}
cv_splits <- wage_train %>%
  vfold_cv(v = 5)
```

### Set a grid for lambda, delta

```{r}
#lambda_grid <- expand_grid()

super_grid <- expand_grid("lambda" = 1:4,"delta" = 1:4,"gamma" = 1:4)

```

### Define a regular linear regression

```{r}
lm_mod <- linear_reg() %>%
  set_engine("lm") %>% 
  set_mode("regression")
```

### Workflow

```{r}
basic_wkflw <- 
  workflow() %>% 
  add_recipe(base_rec) %>% 
  add_model(lm_mod)
```

### Tune

```{r}
basic_lm_reuslts <- basic_wkflw %>% 
  tune_grid(
    resamples= cv_splits,
    grid = super_grid
    )

basic_lm_reuslts %>% show_best(
  metric = "rmse", n=5, maximize=FALSE)
```


### select best lambda
```{r}
best_lambda <- basic_lm_reuslts %>% select_best(metric = "rmse",maximize=FALSE)
best_lambda
```
We can see that the best degree for each predictor is 1.


