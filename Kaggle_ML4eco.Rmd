---
title: "Kaggle_ML4eco"
author: "Gil Omer and Doron Zamir"
date: "6/19/2021"
output: html_document
---

```{r setup, include=FALSE}
#rm(list=ls())
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval    = TRUE,
  echo    = TRUE,
  warning = FALSE,
  message = FALSE,
  cache   = FALSE,
  dev     = "svglite",
  fig.ext = ".svg"
)
```

# Packages and setup

```{r pacman, message=FALSE, warning=FALSE, eval=TRUE}
pacman::p_load(
  tidyverse,   # for data wrangling and visualization
  tidymodels,  # for data modeling
  GGally,      # for pairs plot
  skimr,       # for summary statistics
  here,         # for referencing folders and files
  broom,  # for tidy model output
  rpart, # for estimating CART
  rpart.plot, # for plotting rpart objects
  ranger, # for estimating random forests
  vip, # for variable importance plots
  DataExplorer,
  glmnet,
  ggmosaic
  )

```

## read data

```{r}
wage_train <- 
  here("train.csv") %>% 
  read_csv()
wage_test <- 
  here("test.csv") %>% 
  read_csv()

```

# Data Analasys

## explore data - glimpse

```{r glimpse}
glimpse(wage_train)
wage_train %>%skim()

```

We can see that variables female, college degree, advanced degree,region, race and occupation dummies, should be factors.
In addition, we can see there are no farmers in the train set.

## Histograms

```{r histogram}
plot_histogram(wage_train)

```

We can see that years of experience (full time) is more spread (from 0 to 45) compared to part time (most observations around 0). This can be explained if most people work in part time jobs for a short period (before/after college for example) but move on to full time jobs. 
In addition, we can see a heaps around 12 years education. 
Hence, we want to look at the total education years+exp years (=~age).


## Boxplot of every variable against log wage

```{r boxplot}
plot_boxplot(wage_train,"lnwage")
```

we can see that there is more variance in higher education levels and in very high and small experience (full time). we can see that higher wage is more associated with low part time experience.

# Modeling

## Basic Model

This model should be used as a benchmark

### Basic Recipe

```{r}
base_rec <-
  recipe(lnwage ~., data = wage_train) %>%
  step_rm(expfsq) %>% 
  step_rm(exppsq) %>%
  update_role(ID, new_role = "ID") %>% 
  step_bin2factor(all_predictors(), -starts_with("exp"), -"edyrs" ) %>%
  #step_mutate(age = edyrs + expf + expp) %>%
  step_poly(expp, degree = tune("lambda")) %>%
  step_poly(edyrs, degree = tune("delta")) %>%
  step_poly(expf, degree = tune("gamma")) %>%
  step_center(all_predictors(), - all_nominal()) %>%
  step_scale(all_predictors(), - all_nominal()) %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors())
```

### Set cross - Validation

```{r}
set.seed(123) 
cv_splits <- wage_train %>%
  vfold_cv(v = 5)
```

### Set a grid for lambda, delta

```{r}
#lambda_grid <- expand_grid()
super_grid <- expand_grid("lambda" = 1:4,"delta" = 1:4,"gamma" = 1:4)

```

### Define a regular linear regression

```{r}
lm_mod <- linear_reg() %>%
  set_engine("lm") %>% 
  set_mode("regression")
```

### Workflow

```{r}
basic_wkflw <- 
  workflow() %>% 
  add_recipe(base_rec) %>% 
  add_model(lm_mod)
```

### Tune
Because of the connections between years of education and experience, we tune their polynomial order together.   

```{r}
basic_lm_reuslts <- basic_wkflw %>% 
  tune_grid(
    resamples= cv_splits,
    grid = super_grid
    )

basic_lm_reuslts %>% show_best(
  metric = "rmse", n=5, maximize=FALSE)
```


### select best tune 
```{r}
best_tune <- basic_lm_reuslts %>% select_best(metric = "rmse",maximize=FALSE)
best_tune
```

### use best degree
```{r}
base_final <- base_rec %>% 
  finalize_recipe(best_tune)
```
### apply on training and test
```{r}
base_train <- base_final %>% 
  prep() %>% 
  juice()
base_test <- base_final %>% 
  prep() %>% 
  bake(new_data = wage_test)
```
# Fit the Model to the Training Set

Fit the optimal model to the training set:
```{r fit}
base_fit <- lm_mod %>% 
  fit(lnwage ~ ., data = base_train)
```

Here are the estimated coefficients:
```{r broom_fit}
base_fit %>% tidy()
```
# Make Predictions Using the Test Set

Create a tibble with the predictions and ID
```{r pred}
base_pred <- base_fit %>% 
  predict(new_data = base_test) %>% 
  bind_cols(base_test) %>% mutate(lnwage=.pred)%>% 
  select(ID, lnwage)
head(base_pred)
```
##save table for submission
```{r}
write.csv(x=base_pred, file="test_basesample.csv", row.names = FALSE) 
```

comment: the basic model produced a score of 0.158. 

##adding trees to the model
As we saw above, the relationship between wage and education/experience is not linear, and there is more variation in higher levels of education and experience.  Hence, we will now try  a more complex model by combining trees. 

```{r}
formula_full <- lnwage ~ .
wage_tree_fit <- rpart(formula_full,method="anova", data=base_train,control = rpart.control(cp=0,minsplit = 80)) #we control the number of variables in order to avoid overfitting.
rpart.plot(wage_tree_fit, cex = 1)

```

## tune cp -cross validation
```{r}
printcp(wage_tree_fit)
plotcp(wage_tree_fit)
```

we can see that the first optimal cp is 0.014

#pruning the model

```{r}
tree_prune_wage <- rpart::prune(wage_tree_fit, cp=0.014)
rpart.plot(tree_prune_wage)
```

##variable importance

```{r}
vip(tree_prune_wage)
```
we can see that female and education variables are the most important.

##predict train
```{r}
redict_prune_train <- predict(tree_prune_wage, base_train)
pred_train_tree <- base_train %>% mutate(pred_tree = redict_prune_train) 
#test rmse
pred_train_tree %>% 
  rmse(lnwage, pred_tree)
```

##predict test
```{r}
redict_prune_test <- predict(tree_prune_wage, base_test)
pred_test_tree <- base_test %>% mutate(pred_tree = redict_prune_test) %>% select(ID,pred_tree) %>% mutate(lnwage=pred_tree) %>% select(ID,lnwage)
```

##save table for submission
```{r}
write.csv(x=pred_test_tree, file="test_tree.csv", row.names = FALSE) 
```